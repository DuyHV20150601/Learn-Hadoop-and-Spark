{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang16393{\fonttbl{\f0\froman\fcharset0 Times New Roman;}{\f1\froman Times New Roman;}{\f2\fnil\fcharset0 Calibri;}{\f3\fnil\fcharset2 Symbol;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.10240}\viewkind4\uc1 
\pard\keepn\sb100\sa100\b\f0\fs24 Big Data Hadoop Interview Questions and Answers- These are Hadoop Basic Interview Questions and Answers for freshers and experienced.\par

\pard\sb100\sa100 1. What is Big Data?\b0\par
Any data that cannot be stored into traditional RDBMS is termed as Big Data. As we know most of the data that we use today has been generated in the past 20 years. And this data is mostly unstructured or semi structured in nature. More than the volume of the data \f1\endash\f0  it is the nature of the data that defines whether it is considered as Big Data or not.\par
Here is an interesting and explanatory visual on \ldblquote What is Big Data?\rdblquote\par
\b 2. What do the four V\rquote s of Big Data denote?\b0\par
IBM has a nice, simple explanation for the four critical features of big data:\line a) Volume \f1\endash\f0 Scale of data\line b) Velocity \f1\endash\f0 Different forms of data\line c) Variety \f1\endash\f0 Analysis of streaming data\line d) Veracity \f1\endash\f0 Uncertainty of data\par
Here is an explanatory video on the four V\rquote s of Big Data\par
\b 3. How big data analysis helps businesses increase their revenue? Give example.\b0\par
Big data analysis is helping businesses differentiate themselves \f1\endash\f0  for example Walmart the world\rquote s largest retailer in 2014 in terms of revenue - is using big data analytics to increase its sales through better predictive analytics, providing customized recommendations and launching new products based on customer preferences and needs. Walmart observed a significant 10% to 15% increase in online sales for $1 billion in incremental revenue. There are many more companies like Facebook, Twitter, LinkedIn, Pandora, JPMorgan Chase, Bank of America, etc. using big data analytics to boost their revenue.\par
Here is an interesting video that explains how various industries are leveraging big data analysis to increase their revenue\par
\b 4. Name some companies that use Hadoop.\b0\par
Yahoo (One of the biggest user & more than 80% code contributor to Hadoop)\line Facebook\line Netflix\line Amazon\line Adobe\line eBay\line Hulu\line Spotify\line Rubikloud\line Twitter\par
To view a detailed list of some of the top companies using Hadoop {{\field{\*\fldinst{HYPERLINK "https://www.dezyre.com/article/top-10-industries-using-big-data-and-121-companies-who-hire-hadoop-developers/69"}}{\fldrslt{\ul\cf1\cf1\ul CLICK HERE}}}}\f0\fs24\par
\b 5. Differentiate between Structured and Unstructured data.\b0\par
Data which can be stored in traditional database systems in the form of rows and columns, for example the online purchase transactions can be referred to as Structured Data. Data which can be stored only partially in traditional database systems, for example, data in XML records can be referred to as semi structured data. Unorganized and raw data that cannot be categorized as semi structured or structured data is referred to as unstructured data. Facebook updates, Tweets on Twitter, Reviews, web logs, etc. are all examples of unstructured data.\par
\b 6. On what concept the Hadoop framework works?\b0\par
Hadoop Framework works on the following two core components-\par
1)HDFS \f1\endash\f0  Hadoop Distributed File System is the java based file system for scalable and reliable storage of large datasets. Data in HDFS is stored in the form of blocks and it operates on the Master Slave Architecture.\par
2)Hadoop MapReduce-This is a java based programming paradigm of Hadoop framework that provides scalability across various Hadoop clusters. MapReduce distributes the workload into various tasks that can run in parallel. Hadoop jobs perform 2 separate tasks- job. The map job breaks down the data sets into key-value pairs or tuples. The reduce job then takes the output of the map job and combines the data tuples to into smaller set of tuples. The reduce job is always performed after the map job is executed.\par
Here is a visual that clearly explain the HDFS and Hadoop MapReduce Concepts-\par
\b 7) What are the main components of a Hadoop Application?\b0\par
Hadoop applications have wide range of technologies that provide great advantage in solving complex business problems.\par
Core components of a Hadoop application are-\par
1) Hadoop Common\par
2) HDFS\par
3) Hadoop MapReduce\par
4) YARN\par
Data Access Components are - Pig and Hive\par
Data Storage Component is - HBase\par
Data Integration Components are - Apache Flume, Sqoop, Chukwa\par
Data Management and Monitoring Components are - Ambari, Oozie and Zookeeper.\par
Data Serialization Components are - Thrift and Avro\par
Data Intelligence Components are - Apache Mahout and Drill.\par
\b 8. What is Hadoop streaming?\b0\par
Hadoop distribution has a generic application programming interface for writing Map and Reduce jobs in any desired programming language like Python, Perl, Ruby, etc. This is referred to as Hadoop Streaming. Users can create and run jobs with any kind of shell scripts or executable as the Mapper or Reducers.\par
\b 9. What is the best hardware configuration to run Hadoop?\b0\par
The best configuration for executing Hadoop jobs is dual core machines or dual processors with 4GB or 8GB RAM that use ECC memory. Hadoop highly benefits from using ECC memory though it is not low - end. ECC memory is recommended for running Hadoop because most of the Hadoop users have experienced various checksum errors by using non ECC memory. However, the hardware configuration also depends on the workflow requirements and can change accordingly.\par
\b 10. What are the most commonly defined input formats in Hadoop?\b0\par
The most common Input Formats defined in Hadoop are:\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Text Input Format- This is the default input format defined in Hadoop. \par
{\pntext\f3\'B7\tab}Key Value Input Format- This input format is used for plain text files wherein the files are broken down into lines. \par
{\pntext\f3\'B7\tab}Sequence File Input Format- This input format is used for reading files in sequence.\par

\pard\sb100\sa100 We have further categorized Big Data Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos- 1,2,4,5,6,7,8,9 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos-3,8,9,10\par

\pard\sb100\sa100 {{\field{\*\fldinst{HYPERLINK "https://www.dezyre.com/Hadoop-Training-online/19"}}{\fldrslt{\ul\cf1\cf1\ul Click here to know more about our IBM Certified Hadoop Developer course}}}}\f0\fs24\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop HDFS Interview Questions and Answers \par

\pard\sb100\sa100\fs24 1. What is a block and block scanner in HDFS?\b0\par
Block - The minimum amount of data that can be read or written is generally referred to as a \ldblquote block\rdblquote  in HDFS. The default size of a block in HDFS is 64MB.\par
Block Scanner - Block Scanner tracks the list of blocks present on a DataNode and verifies them to find any kind of checksum errors. Block Scanners use a throttling mechanism to reserve disk bandwidth on the datanode.\par
\b 2. Explain the difference between NameNode, Backup Node and Checkpoint NameNode.\b0\par
\b NameNode\b0 : NameNode is at the heart of the HDFS file system which manages the metadata i.e. the data of the files is not stored on the NameNode but rather it has the directory tree of all the files present in the HDFS file system on a hadoop cluster. NameNode uses two files for the namespace-\par
fsimage file- It keeps track of the latest checkpoint of the namespace.\par
edits file-It is a log of changes that have been made to the namespace since checkpoint.\par
\b Checkpoint Node-\b0\par
Checkpoint Node keeps track of the latest checkpoint in a directory that has same structure as that of NameNode\rquote s directory. Checkpoint node creates checkpoints for the namespace at regular intervals by downloading the edits and fsimage file from the NameNode and merging it locally. The new image is then again updated back to the active NameNode.\par
\b BackupNode:\b0\par
Backup Node also provides check pointing functionality like that of the checkpoint node but it also maintains its up-to-date in-memory copy of the file system namespace that is in sync with the active NameNode.\par
\b 3. What is commodity hardware?\b0\par
Commodity Hardware refers to inexpensive systems that do not have high availability or high quality. Commodity Hardware consists of RAM because there are specific services that need to be executed on RAM. Hadoop can be run on any commodity hardware and does not require any super computer s or high end hardware configuration to execute jobs.\par
\b 4. What is the port number for NameNode, Task Tracker and Job Tracker?\b0\par
NameNode 50070\par
Job Tracker 50030\par
Task Tracker 50060\par
\b 5. Explain about the process of inter cluster data copying.\b0\par
HDFS provides a distributed data copying facility through the DistCP from source to destination. If this data copying is within the hadoop cluster then it is referred to as inter cluster data copying. DistCP requires both source and destination to have a compatible or same version of hadoop.\par
\b 6. How can you overwrite the replication factors in HDFS?\b0\par
The replication factor in HDFS can be modified or overwritten in 2 ways-\par
1)Using the Hadoop FS Shell, replication factor can be changed per file basis using the below command-\par
$hadoop fs \f1\endash\f0 setrep \f1\endash\f0 w 2 /my/test_file (test_file is the filename whose replication factor will be set to 2)\par
2)Using the Hadoop FS Shell, replication factor of all files under a given directory can be modified using the below command-\par
3)$hadoop fs \f1\endash\f0 setrep \f1\endash\f0 w 5 /my/test_dir (test_dir is the name of the directory and all the files in this directory will have a replication factor set to 5)\par
\b 7. Explain the difference between NAS and HDFS.\b0\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 NAS runs on a single machine and thus there is no probability of data redundancy whereas HDFS runs on a cluster of different machines thus there is data redundancy because of the replication protocol. \par
{\pntext\f3\'B7\tab}NAS stores data on a dedicated hardware whereas in HDFS all the data blocks are distributed across local drives of the machines. \par
{\pntext\f3\'B7\tab}In NAS data is stored independent of the computation and hence Hadoop MapReduce cannot be used for processing whereas HDFS works with Hadoop MapReduce as the computations in HDFS are moved to data.\par

\pard\sb100\sa100\b 8. Explain what happens if during the PUT operation, HDFS block is assigned a replication factor 1 instead of the default value 3.\b0\par
Replication factor is a property of HDFS that can be set accordingly for the entire cluster to adjust the number of times the blocks are to be replicated to ensure high data availability. For every block that is stored in HDFS, the cluster will have n-1 duplicated blocks. So, if the replication factor during the PUT operation is set to 1 instead of the default value 3, then it will have a single copy of data. Under these circumstances when the replication factor is set to 1 ,if the DataNode crashes under any circumstances, then only single copy of the data would be lost.\par
\b 9. What is the process to change the files at arbitrary locations in HDFS?\b0\par
HDFS does not support modifications at arbitrary offsets in the file or multiple writers but files are written by a single writer in append only format i.e. writes to a file in HDFS are always made at the end of the file.\par
\b 10. Explain about the indexing process in HDFS.\b0\par
Indexing process in HDFS depends on the block size. HDFS stores the last part of the data that further points to the address where the next part of data chunk is stored.\par
\b 11. What is a rack awareness and on what basis is data stored in a rack?\b0\par
All the data nodes put together form a storage area i.e. the physical location of the data nodes is referred to as Rack in HDFS. The rack information i.e. the rack id of each data node is acquired by the NameNode. The process of selecting closer data nodes depending on the rack information is known as Rack Awareness.\par
The contents present in the file are divided into data block as soon as the client is ready to load the file into the hadoop cluster. After consulting with the NameNode, client allocates 3 data nodes for each data block. For each data block, there exists 2 copies in one rack and the third copy is present in another rack. This is generally referred to as the Replica Placement Policy.\par
We have further categorized Hadoop HDFS Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos- 2,3,7,9,10,11 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,2, 4,5,6,7,8\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop MapReduce Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1. Explain the usage of Context Object.\b0\par
Context Object is used to help the mapper interact with other Hadoop systems. Context Object can be used for updating counters, to report the progress and to provide any application level status updates. ContextObject has the configuration details for the job and also interfaces, that helps it to generating the output.\par
\b 2. What are the core methods of a Reducer?\b0\par
The 3 core methods of a reducer are \f1\endash\f0\par
1)setup () \f1\endash\f0  This method of the reducer is used for configuring various parameters like the input data size, distributed cache, heap size, etc.\par
Function Definition- \i public void setup (context)\i0\par
2)reduce () it is heart of the reducer which is called once per key with the associated reduce task.\par
Function Definition -public void reduce (Key,Value,context)\par
3)cleanup () - This method is called only once at the end of reduce task for clearing all the temporary files.\par
Function Definition -public void cleanup (context)\par
\b 3. Explain about the partitioning, shuffle and sort phase\b0\par
\b Shuffle Phase-\b0 Once the first map tasks are completed, the nodes continue to perform several other map tasks and also exchange the intermediate outputs with the reducers as required. This process of moving the intermediate outputs of map tasks to the reducer is referred to as Shuffling.\par
\b Sort Phase\b0 - Hadoop MapReduce automatically sorts the set of intermediate keys on a single node before they are given as input to the reducer.\par
\b Partitioning Phase-\b0 The process that determines which intermediate keys and value will be received by each reducer instance is referred to as partitioning. The destination partition is same for any key irrespective of the mapper instance that generated it.\par
\b 4. How to write a custom partitioner for a Hadoop MapReduce job?\b0\par
Steps to write a Custom Partitioner for a Hadoop MapReduce Job-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 A new class must be created that extends the pre-defined Partitioner Class. \par
{\pntext\f3\'B7\tab}getPartition method of the Partitioner class must be overridden. \par
{\pntext\f3\'B7\tab}The custom partitioner to the job can be added as a config file in the wrapper which runs Hadoop MapReduce or the custom partitioner can be added to the job by using the set method of the partitioner class.\par

\pard\sb100\sa100\b 5. What is the relationship between Job and Task in Hadoop?\b0\par
A single job can be broken down into one or many tasks in Hadoop.\par
\b 6. Is it important for Hadoop MapReduce jobs to be written in Java?\b0\par
It is not necessary to write Hadoop MapReduce jobs in java but users can write MapReduce jobs in any desired programming language like Ruby, Perl, Python, R, Awk, etc. through the Hadoop Streaming API.\par
\b 7. What is the process of changing the split size if there is limited storage space on Commodity Hardware?\b0\par
If there is limited storage space on commodity hardware, the split size can be changed by implementing the \ldblquote Custom Splitter\rdblquote . The call to Custom Splitter can be made from the main method.\par
\b 8. What are the primary phases of a Reducer?\b0\par
The 3 primary phases of a reducer are \f1\endash\f0\par
1)Shuffle\par
2)Sort\par
3)Reduce\par
\b 9. What is a TaskInstance?\b0\par
The actual hadoop MapReduce jobs that run on each slave node are referred to as Task instances. Every task instance has its own JVM process. For every new task instance, a JVM process is spawned by default for a task.\par
\b 10. Can reducers communicate with each other?\b0\par
Reducers always run in isolation and they can never communicate with each other as per the Hadoop MapReduce programming paradigm.\par
We have further categorized Hadoop MapReduce Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos- 2,5,6 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,3,4,7,8,9,10\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop HBase Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1. When should you use HBase and what are the key components of HBase?\b0\par
HBase should be used when the big data application has \f1\endash\f0\par
1)A variable schema\par
2)When data is stored in the form of collections\par
3)If the application demands key based access to data while retrieving.\par
Key components of HBase are \f1\endash\f0\par
Region- This component contains memory data store and Hfile.\par
Region Server-This monitors the Region.\par
HBase Master-It is responsible for monitoring the region server.\par
Zookeeper- It takes care of the coordination between the HBase Master component and the client.\par
Catalog Tables-The two important catalog tables are ROOT and META.ROOT table tracks where the META table is and META table stores all the regions in the system.\par
\b 2. What are the different operational commands in HBase at record level and table level?\b0\par
Record Level Operational Commands in HBase are \f1\endash\f0 put, get, increment, scan and delete.\par
Table Level Operational Commands in HBase are-describe, list, drop, disable and scan.\par
\b 3. What is Row Key?\b0\par
Every row in an HBase table has a unique identifier known as RowKey. It is used for grouping cells logically and it ensures that all cells that have the same RowKeys are co-located on the same server. RowKey is internally regarded as a byte array.\par
\b 4. Explain the difference between RDBMS data model and HBase data model.\b0\par
RDBMS is a schema based database whereas HBase is schema less data model.\par
RDBMS does not have support for in-built partitioning whereas in HBase there is automated partitioning.\par
RDBMS stores normalized data whereas HBase stores de-normalized data.\par
\b 5. Explain about the different catalog tables in HBase?\b0\par
The two important catalog tables in HBase, are ROOT and META. ROOT table tracks where the META table is and META table stores all the regions in the system.\par
\b 6. What is column families? What happens if you alter the block size of ColumnFamily on an already populated database?\b0\par
The logical deviation of data is represented through a key known as column Family. Column families consist of the basic unit of physical storage on which compression features can be applied. In an already populated database, when the block size of column family is altered, the old data will remain within the old block size whereas the new data that comes in will take the new block size. When compaction takes place, the old data will take the new block size so that the existing data is read correctly.\par
\b 7. Explain the difference between HBase and Hive.\b0\par
HBase and Hive both are completely different hadoop based technologies-Hive is a data warehouse infrastructure on top of Hadoop whereas HBase is a NoSQL key value store that runs on top of Hadoop. Hive helps SQL savvy people to run MapReduce jobs whereas HBase supports 4 primary operations-put, get, scan and delete. HBase is ideal for real time querying of big data where Hive is an ideal choice for analytical querying of data collected over period of time.\par
\b 8. Explain the process of row deletion in HBase.\b0\par
On issuing a delete command in HBase through the HBase client, data is not actually deleted from the cells but rather the cells are made invisible by setting a tombstone marker. The deleted cells are removed at regular intervals during compaction.\par
\b 9. What are the different types of tombstone markers in HBase for deletion?\b0\par
There are 3 different types of tombstone markers in HBase for deletion-\par
1)Family Delete Marker- This markers marks all columns for a column family.\par
2)Version Delete Marker-This marker marks a single version of a column.\par
3)Column Delete Marker-This markers marks all the versions of a column.\par
\b 10. Explain about HLog and WAL in HBase.\b0\par
All edits in the HStore are stored in the HLog. Every region server has one HLog. HLog contains entries for edits of all regions performed by a particular Region Server.WAL abbreviates to Write Ahead Log (WAL) in which all the HLog edits are written immediately.WAL edits remain in the memory till the flush period in case of deferred log flush.\par
We have further categorized Hadoop HBase Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos-1,2,4,5,7 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos-2,3,6,8,9,10\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop Sqoop Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1. Explain about some important Sqoop commands other than import and export.\b0\par
\b Create Job (--create)\b0\par
Here we are creating a job with the name my job, which can import the table data from RDBMS table to HDFS. The following command is used to create a job that is importing data from the employee table in the db database to the HDFS file.\par
$ Sqoop job --create myjob \\\par
--import \\\par
--connect jdbc:mysql://localhost/db \\\par
--username root \\\par
--table employee --m 1\par
\b Verify Job (--list)\b0\par
\lquote --list\rquote  argument is used to verify the saved jobs. The following command is used to verify the list of saved Sqoop jobs.\par
$ Sqoop job --list\par
\b Inspect Job (--show)\b0\par
\lquote --show\rquote  argument is used to inspect or verify particular jobs and their details. The following command and sample output is used to verify a job called myjob.\par
$ Sqoop job --show myjob\par
\b Execute Job (--exec)\b0\par
\lquote --exec\rquote  option is used to execute a saved job. The following command is used to execute a saved job called myjob.\par
$ Sqoop job --exec myjob\par
\b 2. How Sqoop can be used in a Java program?\b0\par
The Sqoop jar in classpath should be included in the java code. After this the method Sqoop.runTool () method must be invoked. The necessary parameters should be created to Sqoop programmatically just like for command line.\par
\b 3. What is the process to perform an incremental data load in Sqoop?\b0\par
The process to perform incremental data load in Sqoop is to synchronize the modified or updated data (often referred as delta data) from RDBMS to Hadoop. The delta data can be facilitated through the incremental load command in Sqoop.\par
Incremental load can be performed by using Sqoop import command or by loading the data into hive without overwriting it. The different attributes that need to be specified during incremental load in Sqoop are-\par
1)Mode (incremental) \f1\endash\f0 The mode defines how Sqoop will determine what the new rows are. The mode can have value as Append or Last Modified.\par
2)Col (Check-column) \f1\endash\f0 This attribute specifies the column that should be examined to find out the rows to be imported.\par
3)Value (last-value) \f1\endash\f0 This denotes the maximum value of the check column from the previous import operation.\par
\b 4. Is it possible to do an incremental import using Sqoop?\b0\par
Yes, Sqoop supports two types of incremental imports-\par
1)Append\par
2)Last Modified\par
To insert only rows Append should be used in import command and for inserting the rows and also updating Last-Modified should be used in the import command.\par
\b 5. What is the standard location or path for Hadoop Sqoop scripts?\b0\par
/usr/bin/Hadoop Sqoop\par
\b 6. How can you check all the tables present in a single database using Sqoop?\b0\par
The command to check the list of all tables present in a single database using Sqoop is as follows-\par
\b Sqoop list-tables \f1\endash\f0 connect jdbc: mysql: //localhost/user;\b0\par
\b 7. How are large objects handled in Sqoop?\b0\par
Sqoop provides the capability to store large sized data into a single field based on the type of data. Sqoop supports the ability to store-\par
1)CLOB \lquote s \f1\endash\f0  Character Large Objects\par
2)BLOB\rquote s \f1\endash\f0 Binary Large Objects\par
Large objects in Sqoop are handled by importing the large objects into a file referred as \ldblquote LobFile\rdblquote  i.e. Large Object File. The LobFile has the ability to store records of huge size, thus each record in the LobFile is a large object.\par
\b 8. Can free form SQL queries be used with Sqoop import command? If yes, then how can they be used?\b0\par
Sqoop allows us to use free form SQL queries with the import command. The import command should be used with the \f1\endash\f0 e and \f1\endash\f0  query options to execute free form SQL queries. When using the \f1\endash\f0 e and \f1\endash\f0 query options with the import command the \f1\endash\f0 target dir value must be specified.\par
\b 9. Differentiate between Sqoop and distCP.\b0\par
DistCP utility can be used to transfer data between clusters whereas Sqoop can be used to transfer data only between Hadoop and RDBMS.\par
\b 10. What are the limitations of importing RDBMS tables into Hcatalog directly?\b0\par
There is an option to import RDBMS tables into Hcatalog directly by making use of \f1\endash\f0 hcatalog \f1\endash\f0 database option with the \f1\endash\f0 hcatalog \f1\endash\f0 table but the limitation to it is that there are several arguments like \f1\endash\f0 as-avrofile , -direct, -as-sequencefile, -target-dir , -export-dir are not supported.\par
We have further categorized Hadoop Sqoop Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos- 4,5,6,9 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1,2,3,6,7,8,10\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop Flume Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1) Explain about the core components of Flume.\b0\par
The core components of Flume are \f1\endash\f0\par
Event- The single log entry or unit of data that is transported.\par
Source- This is the component through which data enters Flume workflows.\par
Sink-It is responsible for transporting data to the desired destination.\par
Channel- it is the duct between the Sink and Source.\par
Agent- Any JVM that runs Flume.\par
Client- The component that transmits event to the source that operates with the agent.\par
\b 2) Does Flume provide 100% reliability to the data flow?\b0\par
Yes, Apache Flume provides end to end reliability because of its transactional approach in data flow.\par
\b 3) How can Flume be used with HBase?\b0\par
Apache Flume can be used with HBase using one of the two HBase sinks \f1\endash\f0\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 HBaseSink (org.apache.flume.sink.hbase.HBaseSink) supports secure HBase clusters and also the novel HBase IPC that was introduced in the version HBase 0.96. \par
{\pntext\f3\'B7\tab}AsyncHBaseSink (org.apache.flume.sink.hbase.AsyncHBaseSink) has better performance than HBase sink as it can easily make non-blocking calls to HBase.\par

\pard\sb100\sa100\b Working of the HBaseSink \f1\endash\b0\f0\par
In HBaseSink, a Flume Event is converted into HBase Increments or Puts. Serializer implements the HBaseEventSerializer which is then instantiated when the sink starts. For every event, sink calls the initialize method in the serializer which then translates the Flume Event into HBase increments and puts to be sent to HBase cluster.\par
\b Working of the AsyncHBaseSink-\b0\par
AsyncHBaseSink implements the AsyncHBaseEventSerializer. The initialize method is called only once by the sink when it starts. Sink invokes the setEvent method and then makes calls to the getIncrements and getActions methods just similar to HBase sink. When the sink stops, the cleanUp method is called by the serializer.\par
\b 4) Explain about the different channel types in Flume. Which channel type is faster?\b0\par
The 3 different built in channel types available in Flume are-\par
MEMORY Channel \f1\endash\f0  Events are read from the source into memory and passed to the sink.\par
JDBC Channel \f1\endash\f0  JDBC Channel stores the events in an embedded Derby database.\par
FILE Channel \f1\endash\f0 File Channel writes the contents to a file on the file system after reading the event from a source. The file is deleted only after the contents are successfully delivered to the sink.\par
MEMORY Channel is the fastest channel among the three however has the risk of data loss. The channel that you choose completely depends on the nature of the big data application and the value of each event.\par
\b 5) Which is the reliable channel in Flume to ensure that there is no data loss?\b0\par
FILE Channel is the most reliable channel among the 3 channels JDBC, FILE and MEMORY.\par
\b 6) Explain about the replication and multiplexing selectors in Flume.\b0\par
Channel Selectors are used to handle multiple channels. Based on the Flume header value, an event can be written just to a single channel or to multiple channels. If a channel selector is not specified to the source then by default it is the Replicating selector. Using the replicating selector, the same event is written to all the channels in the source\rquote s channels list. Multiplexing channel selector is used when the application has to send different events to different channels.\par
\b 7) How multi-hop agent can be setup in Flume?\b0\par
Avro RPC Bridge mechanism is used to setup Multi-hop agent in Apache Flume.\par
\b 8) Does Apache Flume provide support for third party plug-ins?\b0\par
Most of the data analysts use Apache Flume has plug-in based architecture as it can load data from external sources and transfer it to external destinations.\par
\b 9) Is it possible to leverage real time analysis on the big data collected by Flume directly? If yes, then explain how.\b0\par
Data from Flume can be extracted, transformed and loaded in real-time into Apache Solr servers using \i MorphlineSolrSink\i0\par
\b 10) Differentiate between FileSink and FileRollSink\b0\par
The major difference between HDFS FileSink and FileRollSink is that HDFS File Sink writes the events into the Hadoop Distributed File System (HDFS) whereas File Roll Sink stores the events into the local file system.\par
\b Hadoop Flume Interview Questions and Answers for Freshers - Q.Nos- 1,2,4,5,6,10\b0\par
\b Hadoop Flume Interview Questions and Answers for Experienced- Q.Nos- 3,7,8,9\b0\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop Zookeeper Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1) Can Apache Kafka be used without Zookeeper?\b0\par
It is not possible to use Apache Kafka without Zookeeper because if the Zookeeper is down Kafka cannot serve client request.\par
\b 2) Name a few companies that use Zookeeper.\b0\par
Yahoo, Solr, Helprace, Neo4j, Rackspace\par
\b 3) What is the role of Zookeeper in HBase architecture?\b0\par
In HBase architecture, ZooKeeper is the monitoring server that provides different services like \f1\endash\f0 tracking server failure and network partitions, maintaining the configuration information, establishing communication between the clients and region servers, usability of ephemeral nodes to identify the available servers in the cluster.\par
\b 4) Explain about ZooKeeper in Kafka\b0\par
Apache Kafka uses ZooKeeper to be a highly distributed and scalable system. Zookeeper is used by Kafka to store various configurations and use them across the hadoop cluster in a distributed manner. To achieve distributed-ness, configurations are distributed and replicated throughout the leader and follower nodes in the ZooKeeper ensemble. We cannot directly connect to Kafka by bye-passing ZooKeeper because if the ZooKeeper is down it will not be able to serve the client request.\par
\b 5) Explain how Zookeeper works\b0\par
ZooKeeper is referred to as the King of Coordination and distributed applications use ZooKeeper to store and facilitate important configuration information updates. ZooKeeper works by coordinating the processes of distributed applications. ZooKeeper is a robust replicated synchronization service with eventual consistency. A set of nodes is known as an ensemble and persisted data is distributed between multiple nodes.\par
3 or more independent servers collectively form a ZooKeeper cluster and elect a master. One client connects to any of the specific server and migrates if a particular node fails. The ensemble of ZooKeeper nodes is alive till the majority of nods are working. The master node in ZooKeeper is dynamically selected by the consensus within the ensemble so if the master node fails then the role of master node will migrate to another node which is selected dynamically. Writes are linear and reads are concurrent in ZooKeeper.\par
\b 6) List some examples of Zookeeper use cases.\b0\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Found by Elastic uses Zookeeper comprehensively for resource allocation, leader election, high priority notifications and discovery. The entire service of Found built up of various systems that read and write to Zookeeper. \par
{\pntext\f3\'B7\tab}Apache Kafka that depends on ZooKeeper is used by LinkedIn \par
{\pntext\f3\'B7\tab}Storm that relies on ZooKeeper is used by popular companies like Groupon and Twitter.\par

\pard\sb100\sa100\b 7) How to use Apache Zookeeper command line interface?\b0\par
ZooKeeper has a command line client support for interactive use. The command line interface of ZooKeeper is similar to the file and shell system of UNIX. Data in ZooKeeper is stored in a hierarchy of Znodes where each znode can contain data just similar to a file. Each znode can also have children just like directories in the UNIX file system.\par
Zookeeper-client command is used to launch the command line client. If the initial prompt is hidden by the log messages after entering the command, users can just hit ENTER to view the prompt.\par
\b 8) What are the different types of Znodes?\b0\par
There are 2 types of Znodes namely- Ephemeral and Sequential Znodes.\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 The Znodes that get destroyed as soon as the client that created it disconnects are referred to as Ephemeral Znodes. \par
{\pntext\f3\'B7\tab}Sequential Znode is the one in which sequential number is chosen by the ZooKeeper ensemble and is pre-fixed when the client assigns name to the znode.\par

\pard\sb100\sa100\b 9) What are watches?\b0\par
Client disconnection might be troublesome problem especially when we need to keep a track on the state of Znodes at regular intervals. ZooKeeper has an event system referred to as watch which can be set on Znode to trigger an event whenever it is removed, altered or any new children are created below it.\par
\b 10) What problems can be addressed by using Zookeeper?\b0\par
In the development of distributed systems, creating own protocols for coordinating the hadoop cluster results in failure and frustration for the developers. The architecture of a distributed system can be prone to deadlocks, inconsistency and race conditions. This leads to various difficulties in making the hadoop cluster fast, reliable and scalable. To address all such problems, Apache ZooKeeper can be used as a coordination service to write correct distributed applications without having to reinvent the wheel from the beginning.\par
\b Hadoop ZooKeeper Interview Questions and Answers for Freshers - Q.Nos- 1,2,8,9\b0\par
\b Hadoop ZooKeeper Interview Questions and Answers for Experienced- Q.Nos-3,4,5,6,7, 10\b0\par
\b Hadoop Pig Interview Questions and Answers\b0\par
\b 1) What do you mean by a bag in Pig?\b0\par
Collection of tuples is referred as a bag in Apache Pig\par
\b 2) Does Pig support multi-line commands?\b0\par
Yes\par
\b 3) What are different modes of execution in Apache Pig?\b0\par
Apache Pig runs in 2 modes- one is the \ldblquote Pig (Local Mode) Command Mode\rdblquote  and the other is the \ldblquote Hadoop MapReduce (Java) Command Mode\rdblquote . Local Mode requires access to only a single machine where all files are installed and executed on a local host whereas MapReduce requires accessing the Hadoop cluster.\par
\b 4) Explain the need for MapReduce while programming in Apache Pig.\b0\par
Apache Pig programs are written in a query language known as Pig Latin that is similar to the SQL query language. To execute the query, there is need for an execution engine. The Pig engine converts the queries into MapReduce jobs and thus MapReduce acts as the execution engine and is needed to run the programs.\par
\b 5) Explain about co-group in Pig.\b0\par
COGROUP operator in Pig is used to work with multiple tuples. COGROUP operator is applied on statements that contain or involve two or more relations. The COGROUP operator can be applied on up to 127 relations at a time. When using the COGROUP operator on two tables at once-Pig first groups both the tables and after that joins the two tables on the grouped columns.\par
\b 6) Explain about the BloomMapFile.\b0\par
BloomMapFile is a class that extends the MapFile class. It is used n HBase table format to provide quick membership test for the keys using dynamic bloom filters.\par
\b 7) Differentiate between Hadoop MapReduce and Pig\b0\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Pig provides higher level of abstraction whereas MapReduce provides low level of abstraction. \par
{\pntext\f3\'B7\tab}MapReduce requires the developers to write more lines of code when compared to Apache Pig. \par
{\pntext\f3\'B7\tab}Pig coding approach is comparatively slower than the fully tuned MapReduce coding approach.\par

\pard\sb100\sa100 Read More in Detail- {{\field{\*\fldinst{HYPERLINK http://www.dezyre.com/article/-mapreduce-vs-pig-vs-hive/163 }}{\fldrslt{http://www.dezyre.com/article/-mapreduce-vs-pig-vs-hive/163\ul0\cf0}}}}\f0\fs24\par
\b 8) What is the usage of foreach operation in Pig scripts?\b0\par
FOREACH operation in Apache Pig is used to apply transformation to each element in the data bag so that respective action is performed to generate new data items.\par
Syntax- FOREACH data_bagname GENERATE exp1, exp2\par
\b 9) Explain about the different complex data types in Pig.\b0\par
Apache Pig supports 3 complex data types-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Maps- These are key, value stores joined together using #. \par
{\pntext\f3\'B7\tab}Tuples- Just similar to the row in a table where different items are separated by a comma. Tuples can have multiple attributes. \par
{\pntext\f3\'B7\tab}Bags- Unordered collection of tuples. Bag allows multiple duplicate tuples.\par

\pard\sb100\sa100\u8203?10) \b What does Flatten do in Pig? \b0\par
Sometimes there is data in a tuple or bag and if we want to remove the level of nesting from that data then Flatten modifier in Pig can be used. Flatten un-nests bags and tuples. For tuples, the Flatten operator will substitute the fields of a tuple in place of a tuple whereas un-nesting bags is a little complex because it requires creating new tuples.\par
We have further categorized Hadoop Pig Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos-1,2,4,7,9 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos- 3,5,6,8,10\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop Hive Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1) What is a Hive Metastore? \b0\par
Hive Metastore is a central repository that stores metadata in external database.\par
\b 2) Are multiline comments supported in Hive? \b0\par
No\par
\b 3) What is ObjectInspector functionality?\b0\par
ObjectInspector is used to analyze the structure of individual columns and the internal structure of the row objects. ObjectInspector in Hive provides access to complex objects which can be stored in multiple formats.\par
\b Hadoop Hive Interview Questions and Answers for Freshers- Q.Nos-1,2,3 \b0\par

\pard\keepn\sb100\sa100\b\fs28 Hadoop YARN Interview Questions and Answers\par

\pard\sb100\sa100\fs24 1)What are the stable versions of Hadoop?\b0\par
Release 2.7.1 (stable)\par
Release 2.4.1\par
Release 1.2.1 (stable)\par
\b 2) What is Apache Hadoop YARN?\b0\par
YARN is a powerful and efficient feature rolled out as a part of Hadoop 2.0.YARN is a large scale distributed system for running big data applications.\par
\b 3) Is YARN a replacement of Hadoop MapReduce?\b0\par
YARN is not a replacement of Hadoop but it is a more powerful and efficient technology that supports MapReduce and is also referred to as Hadoop 2.0 or MapReduce 2.\par
We have further categorized Hadoop YARN Interview Questions for Freshers and Experienced-\par

\pard{\pntext\f3\'B7\tab}{\*\pn\pnlvlblt\pnf3\pnindent360{\pntxtb\'B7}}\fi-360\li720\sb100\sa100 Hadoop Interview Questions and Answers for Freshers - Q.Nos- 2,3 \par
{\pntext\f3\'B7\tab}Hadoop Interview Questions and Answers for Experienced - Q.Nos- 1\par

\pard\sa200\sl276\slmult1\f2\fs22\lang9\par
}
 